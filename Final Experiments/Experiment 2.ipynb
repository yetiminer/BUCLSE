{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyna Q model, CVAE, reward model and terminal model given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this allows relative imports in notebook\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Experiment2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UCLSE.dyna_q.Experiment1a import Experiment\n",
    "from UCLSE.dyna_q.dyna_q import TabularMemory\n",
    "from UCLSE.dyna_q.benchmarking import BenchmarkAgent, SpoofAgent, DoNothing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import visdom\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "vis=visdom.Visdom(port=8097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_oracle(observation,cutoff=50,ub=6,lb=-2,lamb=0.5):\n",
    "\n",
    "\n",
    "    distance=observation.distance\n",
    "    inventory=observation.inventory\n",
    "    orders_out=observation.orders_out\n",
    "    bid_change=observation.bid_change\n",
    "    bid_ask_spread=observation.bid_ask_spread\n",
    "    time_left=observation.time_left\n",
    "\n",
    "    ans=lamb*bid_change\n",
    "\n",
    "    if inventory==0:   #terminal            \n",
    "            \n",
    "            ans+=-(1-lamb)*distance\n",
    "            ans-=lamb*bid_change\n",
    "\n",
    "    elif inventory>1: #terminal\n",
    "            \n",
    "            ans+=-bid_ask_spread*(inventory-1)\n",
    "            ans+=-(1-lamb)*distance\n",
    "            ans+=-1 #penalty\n",
    "           \n",
    "    else:\n",
    "\n",
    "            if orders_out>0: \n",
    "                ans+=1/250\n",
    "\n",
    "\n",
    "            if time_left==1: #terminal takes account of exit spread\n",
    "               \n",
    "                ans+=-(1-lamb)*distance\n",
    "\n",
    "            if -distance>=ub:\n",
    "                \n",
    "                ans+=-(1-lamb)*distance\n",
    "              \n",
    "\n",
    "            elif -distance<lb:\n",
    "               \n",
    "                ans+=-(1-lamb)*distance\n",
    "               \n",
    "\n",
    "    return ans \n",
    "\n",
    "def done_oracle(observation,cutoff=50,lb=-2,ub=6):\n",
    "\n",
    "    distance=observation.distance\n",
    "    inventory=observation.inventory\n",
    "    orders_out=observation.orders_out\n",
    "    time_left=observation.time_left\n",
    "\n",
    "    if inventory==0:\n",
    "        done=1\n",
    "        why=f'inventory {inventory}=0'\n",
    "    elif time_left>=1:\n",
    "        done=1\n",
    "        why=f'time up {time_left}'\n",
    "    elif inventory>1:\n",
    "        done=1\n",
    "        why=f'inventory {inventory}>1'\n",
    "    elif -distance>=ub:\n",
    "        done=1\n",
    "        why=f'-distance {distance} >ub {ub}'\n",
    "    elif -distance<lb: \n",
    "        done=1\n",
    "        why=f'-distance {distance}<lb {lb}'\n",
    "\n",
    "    else:\n",
    "        done=0 \n",
    "        why=None\n",
    "    return done,why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trader_pref_kwargs={'qty_min':-5,'qty_max':5,'sigma_pv':1}\n",
    "timer_kwargs={'start':0,'end':6000,'step':1}\n",
    "price_sequence_kwargs={'kappa':0.0002,'mean':100,'sigma':1,'block_length':10}\n",
    "noise_kwargs={'sigma':1}\n",
    "messenger_kwargs={'logging':True}\n",
    "env_kwargs={'trader_arrival_rate':1,'recording':True,#'process_verbose':False,\n",
    "                'bookkeep_verbose':False, 'lob_verbose':False}\n",
    "sigma_n=5\n",
    "\n",
    "def cont_coef():\n",
    "    return np.random.uniform(0.2,0.8)\n",
    "\n",
    "def personal_memory():\n",
    "    return int(np.random.uniform(5,15))\n",
    "\n",
    "trader_kwargs={'ZIP':{'prefix':'ZIP','number':10,'object_name':'WW_Zip',\n",
    "                          'setup_kwargs':\n",
    "                                {'market_make':True,'prior':(100,sigma_n)}},\n",
    "               'HBL':{'prefix':'HBL','number':10,'object_name':'HBL',\n",
    "                          'setup_kwargs':\n",
    "                              {'memory':100,'grace_period':20}},\n",
    "               'CON':{'prefix':'CON','number':10,'object_name':'ContTrader',\n",
    "                      'setup_kwargs':\n",
    "                          {'cont_coeff':cont_coef,'personal_memory':personal_memory,'profit_target':4, 'market_make':True,\n",
    "                          'prior':(100,sigma_n)}},\n",
    "               'NOI':{'prefix':'NOI','number':10,'object_name':'NoiseTrader',\n",
    "                          'setup_kwargs':{'memory':20}}\n",
    "              }\n",
    "\n",
    "lb=-1\n",
    "lamb=0.5\n",
    "ub=10\n",
    "cutoff=100\n",
    "\n",
    "array_reward=array_reward_wrap(lb=loss_limit,lamb=lamb,ub=profit_target)\n",
    "array_done=array_done_wrap(lb=loss_limit,ub=profit_target)\n",
    "\n",
    "lobenv_kwargs={'cutoff':100,'profit_target':ub,'loss_limit':lb,'reward_func':reward_oracle,'lamb':0.5}\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_reward(observation):\n",
    "    \n",
    "    tensor=False\n",
    "    if type(observation)==torch.Tensor:\n",
    "        tensor=True\n",
    "        reward=torch.zeros((observation.shape[0]),device=device,requires_grad = False)\n",
    "        observation=observation.detach()\n",
    "    elif type(observation) in (np.array,np.ndarray):\n",
    "        reward=np.zeros((observation.shape[0]))\n",
    "    #print('rw shape',reward.shape, 'obs shape',observation.shape)\n",
    "\n",
    "    distance=observation[:,0]\n",
    "    inventory=observation[:,1]\n",
    "    orders_out=observation[:,2]\n",
    "    bid_change=observation[:,3]\n",
    "    bid_ask_spread=observation[:,5]\n",
    "    time_left=observation[:,8]\n",
    "    \n",
    "    #print('bc shape', bid_change.shape)\n",
    "\n",
    "    reward+=lamb*bid_change*10\n",
    "\n",
    "    invent_0=inventory==0   #terminal                        \n",
    "    reward[invent_0]+=-(1-lamb)*10*distance[invent_0]\n",
    "    reward[invent_0]-=lamb*10*bid_change[invent_0]\n",
    "\n",
    "    invent_1=inventory>1 #terminal    \n",
    "    reward[invent_1]+=-10*bid_ask_spread[invent_1]*(inventory[invent_1]-1)\n",
    "    reward[invent_1]+=-(1-lamb)*10*distance[invent_1]\n",
    "    reward[invent_1]+=-1 #penalty\n",
    "\n",
    "   #else....\n",
    "    invent_perfect=(~invent_0&~invent_1)\n",
    "\n",
    "\n",
    "    #if orders_out>0: \n",
    "    if_orders_out=orders_out>0\n",
    "    if_orders_out=if_orders_out&invent_perfect\n",
    "    reward[if_orders_out]+=1/250\n",
    "\n",
    "\n",
    "    #if time_left==1: #terminal takes account of exit spread\n",
    "    if_time_left=time_left==1\n",
    "    if_time_left=if_time_left&invent_perfect\n",
    "\n",
    "    reward[if_time_left]+=-(1-lamb)*10*distance[if_time_left]\n",
    "\n",
    "    #if -distance>=ub/10:\n",
    "    if_distance_gr=-distance>ub/10\n",
    "    if_distance_gr=if_distance_gr&invent_perfect\n",
    "    reward[if_distance_gr]+=-(1-lamb)*10*distance[if_distance_gr]\n",
    "\n",
    "    #elif -distance<lb/10:\n",
    "    if_distance_ls=-distance<lb/10\n",
    "    if_distance_ls=if_distance_ls&invent_perfect\n",
    "    reward[if_distance_ls]+=-(1-lamb)*10*distance[if_distance_ls]\n",
    "\n",
    "    if tensor: reward=reward.unsqueeze(1)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def array_done(x):\n",
    "    \n",
    "    tensor=False\n",
    "    if type(x)==torch.Tensor:\n",
    "        tensor=True\n",
    "        done=torch.zeros((x.shape[0]),device=device,requires_grad = False)\n",
    "    elif type(x) in (np.array,np.ndarray):\n",
    "        done=np.zeros((x.shape[0]))\n",
    "\n",
    "    #inventory\n",
    "    done[x[:,1]==0]=1\n",
    "\n",
    "    #time_left\n",
    "    done[x[:,-1]>=1]=1\n",
    "\n",
    "    #inventory\n",
    "    done[x[:,1]>1]=1\n",
    "\n",
    "    #distance\n",
    "    done[-x[:,0]>ub/10]=1\n",
    "\n",
    "    #distance\n",
    "    done[-x[:,0]<lb/10]=1\n",
    "    \n",
    "    if tensor: done=done.unsqueeze(1)\n",
    "\n",
    "    return done\n",
    "\n",
    "agent_kwargs={'CVAE':True,'Q_H1Size':16,'Q_H2Size':16,\n",
    "                   'doneModel':array_done,'rewardModel':array_reward,'loss_func':None,'device':device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyna_config={\n",
    "\t\"double_q_model\": False,\n",
    "\t\"batch_size\": 64,\n",
    "\t\"learning_rate\": 5e-3,\n",
    "\t\"exploration\": {\n",
    "\t\t\"type\": \"exponential\",\n",
    "\t\t\"init_epsilon\": 0.8,\n",
    "\t\t\"min_epsilon\": 0.05,\n",
    "\t\t\"decay_steps\": 100000,\n",
    "\t\t\"decay_eps\": 0.99,\n",
    "        \"choice\":'least_bonus'\n",
    "\t},\n",
    "\t\"memory\": {\n",
    "\t\t\"memory_capacity\": 1000000,\n",
    "\t\t\"prioritized\": False,\n",
    "        \"tabular memory\":True,\n",
    "        \n",
    "\t},\n",
    "\t\"discount\": 0.99,\n",
    "\t\"target_update_freq\": 50,\n",
    "\t\"first_update\": 200,\n",
    "\t\"modify_reward\": False,\n",
    "    \"learn\":'Q',\n",
    "    'double_q_model':True,\n",
    "    'model_update_freq':5,\n",
    "    'planning_freq':5,\n",
    "    'model':'CVAE'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Device in use is  cuda\n",
      "custom reward function\n",
      "custom done function\n",
      "setup tabular memory\n"
     ]
    }
   ],
   "source": [
    "experiment=Experiment(trader_pref_kwargs=trader_pref_kwargs,timer_kwargs=timer_kwargs,\n",
    "           price_sequence_kwargs=price_sequence_kwargs,noise_kwargs=noise_kwargs,\n",
    "           messenger_kwargs=messenger_kwargs,env_kwargs=env_kwargs,trader_kwargs=trader_kwargs,\n",
    "           lobenv_kwargs=lobenv_kwargs,agent_kwargs=agent_kwargs,visdom=vis,dyna_kwargs=dyna_config,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.array_reward(observation)>,\n",
       " <function __main__.array_done(x)>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.agent.env_model.decoder.reward,experiment.agent.env_model.decoder.done,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns,memory=Experiment.memory_returns_loader(experiment)\n",
    "project=array_reward(memory.iloc[:,-9:].values)\n",
    "dones=array_done(memory.iloc[:,-9:].values)\n",
    "print(memory[(memory.done!=dones)])\n",
    "memory[(memory.rw!=project)].rw-project[(memory.rw!=project)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.new_train_setup(MaxEpisodes=1000,planning_steps=1,lookback=30,thresh=3,planning=True,graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x14f23932448>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning is True, double Q model is True, tabular memory is True\n",
      "Dyna-Q - EXP: 1 | Ep: 21 | timestep: 2 | Ep_r:  0.00396 Profit: -1 Avg loss:-0.17317739428728196\n",
      "Dyna-Q - EXP: 1 | Ep: 41 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:-0.06501465333333334\n",
      "Dyna-Q - EXP: 1 | Ep: 61 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:-0.06537896013333332\n",
      "Saving best checkpoint at episode 72 with reward 0\n",
      "Dyna-Q - EXP: 1 | Ep: 81 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.000528\n",
      "Dyna-Q - EXP: 1 | Ep: 101 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.000264\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 121 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.024881999999999998\n",
      "Saving best checkpoint at episode 123 with reward 0.000528\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 141 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.0165\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 161 | timestep: 51 | Ep_r:  1.845931881761253 Profit: 1 Avg loss:-0.14940695852376262\n",
      "Dyna-Q - EXP: 1 | Ep: 181 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:-0.4902083269292543\n",
      "copying eval net to target net\n",
      "Saving best checkpoint at episode 197 with reward 0.024882\n",
      "Dyna-Q - EXP: 1 | Ep: 201 | timestep: 7 | Ep_r:  2.0017615959999997 Profit: 1 Avg loss:0.1166492526882521\n",
      "Dyna-Q - EXP: 1 | Ep: 221 | timestep: 2 | Ep_r:  0.00396 Profit: -1 Avg loss:0.01722368036798759\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 241 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.016703256103141096\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 261 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.041416666666666664\n",
      "Saving best checkpoint at episode 274 with reward 0.0516312225838911\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 281 | timestep: 14 | Ep_r:  0.9626003760976647 Profit: 0 Avg loss:0.13449543626849186\n",
      "Dyna-Q - EXP: 1 | Ep: 301 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.20278702380494165\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 321 | timestep: 2 | Ep_r:  0.0 Profit: -1 Avg loss:-0.002302520373561164\n",
      "Saving best checkpoint at episode 334 with reward 0.0604719577305697\n",
      "Dyna-Q - EXP: 1 | Ep: 341 | timestep: 16 | Ep_r:  0.055416891562049736 Profit: -1 Avg loss:0.10191935169268371\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 361 | timestep: 2 | Ep_r:  0.00396 Profit: -1 Avg loss:0.15100274768926036\n",
      "Dyna-Q - EXP: 1 | Ep: 381 | timestep: 20 | Ep_r:  -1.7421196 Profit: -4 Avg loss:0.05425361866353932\n",
      "Saving best checkpoint at episode 390 with reward 0.08071417363059537\n",
      "Dyna-Q - EXP: 1 | Ep: 401 | timestep: 18 | Ep_r:  3.0283844954199646 Profit: 2 Avg loss:0.13829101489487927\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 421 | timestep: 2 | Ep_r:  0.0 Profit: -1 Avg loss:0.24278384881168782\n",
      "Dyna-Q - EXP: 1 | Ep: 441 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:-0.09308516071465295\n",
      "copying eval net to target net\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 461 | timestep: 2 | Ep_r:  0.0 Profit: -1 Avg loss:-0.17709027120367093\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Saving best checkpoint at episode 471 with reward 0.0983655492503072\n",
      "Dyna-Q - EXP: 1 | Ep: 481 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.1007925946959068\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 501 | timestep: 42 | Ep_r:  1.1683856161254096 Profit: 0 Avg loss:-0.13258182684188066\n",
      "Dyna-Q - EXP: 1 | Ep: 521 | timestep: 6 | Ep_r:  -2.0 Profit: -3 Avg loss:-0.41425440140293995\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 541 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:-0.04060860544172037\n",
      "Saving best checkpoint at episode 551 with reward 0.1007925946959068\n",
      "Dyna-Q - EXP: 1 | Ep: 561 | timestep: 14 | Ep_r:  0.08519557116715534 Profit: -6 Avg loss:0.22444976893669735\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 581 | timestep: 16 | Ep_r:  1.0504168915620498 Profit: -2 Avg loss:0.570364526722026\n",
      "Dyna-Q - EXP: 1 | Ep: 601 | timestep: 20 | Ep_r:  3.0538372249611077 Profit: 2 Avg loss:0.183767980951998\n",
      "Saving best checkpoint at episode 602 with reward 0.12275508338934382\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 621 | timestep: 2 | Ep_r:  0.0 Profit: -1 Avg loss:0.4567852964622411\n",
      "Dyna-Q - EXP: 1 | Ep: 641 | timestep: 1 | Ep_r:  0.2475 Profit: 0 Avg loss:0.2943907554874657\n",
      "copying eval net to target net\n",
      "Saving best checkpoint at episode 655 with reward 0.18514929928990656\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 661 | timestep: 40 | Ep_r:  -1.6215887034278722 Profit: -1 Avg loss:0.11162418539632407\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 681 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.3601202100563021\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 701 | timestep: 20 | Ep_r:  1.0638372249611077 Profit: 0 Avg loss:-0.07237026841459691\n",
      "Saving best checkpoint at episode 719 with reward 0.20145502192124798\n",
      "Dyna-Q - EXP: 1 | Ep: 721 | timestep: 2 | Ep_r:  1.99396 Profit: 1 Avg loss:0.21831436466867807\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 741 | timestep: 21 | Ep_r:  1.0328616827150183 Profit: 0 Avg loss:0.865206091305337\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 761 | timestep: 9 | Ep_r:  0.995 Profit: -2 Avg loss:0.1630264699044564\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 781 | timestep: 11 | Ep_r:  1.017719583874951 Profit: -2 Avg loss:-0.14085460512727607\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 801 | timestep: 21 | Ep_r:  1.0671088527114967 Profit: 0 Avg loss:-0.269770862216369\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 821 | timestep: 25 | Ep_r:  1.0798714562403413 Profit: 0 Avg loss:-0.21408459908550367\n",
      "Dyna-Q - EXP: 1 | Ep: 841 | timestep: 36 | Ep_r:  1.1124347127801706 Profit: 0 Avg loss:-0.8293203189608065\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 861 | timestep: 16 | Ep_r:  1.0504168915620498 Profit: 0 Avg loss:-0.09229348761296013\n",
      "Saving best checkpoint at episode 866 with reward 0.254281789391276\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 881 | timestep: 19 | Ep_r:  -1.4344674495342347 Profit: 37 Avg loss:0.6942172934592687\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 901 | timestep: 6 | Ep_r:  -1.4805920597604 Profit: -3 Avg loss:0.3123704029885036\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 921 | timestep: 18 | Ep_r:  0.06219449541996495 Profit: -1 Avg loss:0.15928413326634597\n",
      "Saving best checkpoint at episode 925 with reward 0.33349432185792016\n",
      "Dyna-Q - EXP: 1 | Ep: 941 | timestep: 2 | Ep_r:  0.00396 Profit: -3 Avg loss:0.6718051203834697\n",
      "copying eval net to target net\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyna-Q - EXP: 1 | Ep: 961 | timestep: 10 | Ep_r:  3.0192471699964782 Profit: 2 Avg loss:-0.08990462153117622\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 981 | timestep: 9 | Ep_r:  -1.9694068989934563 Profit: -5 Avg loss:-0.5076540104348104\n",
      "copying eval net to target net\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Saving best checkpoint at episode 993 with reward 0.337405945082431\n",
      "Dyna-Q - EXP: 1 | Ep: 1001 | timestep: 17 | Ep_r:  -1.4411772773535707 Profit: -1 Avg loss:0.5565715428308261\n",
      "Saving checkpoint at episode 1000\n",
      "Dyna-Q - EXP: 1 | Ep: 1021 | timestep: 4 | Ep_r:  0.011761596 Profit: -3 Avg loss:0.33068687098565774\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 1041 | timestep: 33 | Ep_r:  -2.64109221303931 Profit: -2 Avg loss:-0.07933695578309988\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 1061 | timestep: 16 | Ep_r:  -4.69458310843795 Profit: -1 Avg loss:0.13959273087414642\n",
      "copying eval net to target net\n",
      "Saving best checkpoint at episode 1070 with reward 0.3931830896778944\n",
      "Dyna-Q - EXP: 1 | Ep: 1081 | timestep: 3 | Ep_r:  0.995 Profit: 0 Avg loss:0.08762468173775523\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 1101 | timestep: 11 | Ep_r:  0.995 Profit: 0 Avg loss:0.41588582156596327\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 1121 | timestep: 42 | Ep_r:  -1.3662636882296575 Profit: -1 Avg loss:0.24950722388322696\n",
      "Dyna-Q - EXP: 1 | Ep: 1141 | timestep: 17 | Ep_r:  2.0488227226464293 Profit: 1 Avg loss:0.4631093839283634\n",
      "Saving best checkpoint at episode 1140 with reward 0.430333081515026\n",
      "copying eval net to target net\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 1161 | timestep: 15 | Ep_r:  0.05197665814348459 Profit: -1 Avg loss:0.042695227628186665\n",
      "Dyna-Q - EXP: 1 | Ep: 1181 | timestep: 16 | Ep_r:  2.04541689156205 Profit: 1 Avg loss:0.2952502394169796\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 1201 | timestep: 41 | Ep_r:  0.12750583058937168 Profit: -1 Avg loss:0.4007770110285911\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 1221 | timestep: 7 | Ep_r:  1.014253460837204 Profit: 0 Avg loss:0.19772700687299852\n",
      "copying eval net to target net\n",
      "Dyna-Q - EXP: 1 | Ep: 1241 | timestep: 2 | Ep_r:  0.0 Profit: -1 Avg loss:-0.26365607337039704\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP: 1 | Ep: 1261 | timestep: 1 | Ep_r:  0.0 Profit: -1 Avg loss:0.14193676440902614\n",
      "copying eval net to target net\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-b9e88d34d8b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxEpisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Results/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\Experiment1a.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, MaxEpisodes, start_episode, total_steps, folder)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m                                                 \u001b[1;31m# store current transition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\dyna_q.py\u001b[0m in \u001b[0;36mstore_transition\u001b[1;34m(self, s, a, r, s_, d, initial, test)\u001b[0m\n\u001b[0;32m    351\u001b[0m                         \u001b[1;31m# replace the old memory with new memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_counter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'memory'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'memory_capacity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment.train(MaxEpisodes=10001,start_episode=experiment.episode,folder='Results/'+experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'Results/Experiment2\\dyna_best.pth.tar'\n",
      "=> loaded checkpoint 'Results/Experiment2\\dyna_best.pth.tar' (epoch 1141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micro_zo50ceu\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\Experiment1a.py:799: UserWarning: not double Q\n",
      "  warnings.warn('not double Q')\n",
      "C:\\Users\\micro_zo50ceu\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\Experiment1a.py:816: UserWarning: no optimizer saved\n",
      "  warnings.warn('no optimizer saved')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys unused in checkpoint data:  ['episode', 'setup']\n"
     ]
    }
   ],
   "source": [
    "experiment.resume(exp=experiment,best=True,folder='Results/'+experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use is  cpu\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Device in use is  cuda\n",
      "custom reward function\n",
      "custom done function\n",
      "setup tabular memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micro_zo50ceu\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\Experiment1a.py:555: UserWarning: no eval net for agent, skipping\n",
      "  warnings.warn('no eval net for agent, skipping')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyna-Q - EXP 1, | Ep: , 1, | timestep:  12 | Ep_r: -1.7085539486864518|profit:-3 start:251|end:263\n",
      "r0 0.5\n",
      "Dyna-Q - EXP 1, | Ep: , 26, | timestep:  40 | Ep_r: -2.621588703427872|profit:-3 start:3447|end:3487\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "r0 1.0\n",
      "Dyna-Q - EXP 1, | Ep: , 51, | timestep:  47 | Ep_r: -3.60341015795648|profit:-2 start:1891|end:1938\n",
      "r0 0.5\n",
      "r0 0.5\n",
      "r0 0.5\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP 1, | Ep: , 76, | timestep:  37 | Ep_r: -1.6297796343476312|profit:-3 start:995|end:1032\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP 1, | Ep: , 101, | timestep:  16 | Ep_r: 1.0504168915620498|profit:0 start:206|end:222\n",
      "r0 0.5\n",
      "r0 0.5\n",
      "Dyna-Q - EXP 1, | Ep: , 126, | timestep:  17 | Ep_r: 1.0538227226464292|profit:0 start:3951|end:3968\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "Dyna-Q - EXP 1, | Ep: , 151, | timestep:  14 | Ep_r: 1.0435016748924086|profit:0 start:2755|end:2769\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n",
      "r0 0.5\n",
      "r0 0.5\n",
      "Dyna-Q - EXP 1, | Ep: , 176, | timestep:  31 | Ep_r: 1.2811544945518403|profit:1 start:1027|end:1058\n",
      "Dyna-Q - EXP 1, | Ep: , 201, | timestep:  5 | Ep_r: 2.00560398004|profit:1 start:4564|end:4569\n",
      "sequence made\n",
      "adding exchange to RL trader  RL\n",
      "adding exchange to RL trader  RL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-23dbdd399b7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxEpisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\Experiment1a.py\u001b[0m in \u001b[0;36mtest_setup\u001b[1;34m(self, lobenv_kwargs, MaxEpisodes, agent)\u001b[0m\n\u001b[0;32m    555\u001b[0m                                         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no eval net for agent, skipping'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrwd_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxEpisodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMaxEpisodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\Experiment1a.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, MaxEpisodes, start_episode, agent, testm)\u001b[0m\n\u001b[0;32m    589\u001b[0m                                         \u001b[0mtotal_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m                                         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                                         \u001b[1;31m# take action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\dyna_q.py\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, x, EPSILON)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[1;31m# input only one sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# greedy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                         \u001b[0mactions_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m                         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_a_shape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_a_shape\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# return the argmax index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University College London\\BUCLSE\\UCLSE\\dyna_q\\dyna_q.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mactions_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mactions_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym2\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment.agent.set_device('cpu')\n",
    "experiment.test_setup(MaxEpisodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax,d=experiment.plot_hist_profit(title='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2375.950450</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>2929.166667</td>\n",
       "      <td>110.500000</td>\n",
       "      <td>0.190380</td>\n",
       "      <td>-0.549550</td>\n",
       "      <td>0.096847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1497.185067</td>\n",
       "      <td>1499.178146</td>\n",
       "      <td>1719.906639</td>\n",
       "      <td>64.230055</td>\n",
       "      <td>1.868504</td>\n",
       "      <td>1.547082</td>\n",
       "      <td>0.017514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.691177</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1098.750000</td>\n",
       "      <td>1131.500000</td>\n",
       "      <td>1375.500000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>-1.644746</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2239.000000</td>\n",
       "      <td>2250.500000</td>\n",
       "      <td>3068.000000</td>\n",
       "      <td>110.500000</td>\n",
       "      <td>1.036446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3686.500000</td>\n",
       "      <td>3704.000000</td>\n",
       "      <td>4505.750000</td>\n",
       "      <td>165.750000</td>\n",
       "      <td>1.133306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5102.000000</td>\n",
       "      <td>5116.000000</td>\n",
       "      <td>5561.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>4.067983</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2           3           4  \\\n",
       "count   222.000000   222.000000   222.000000  222.000000  222.000000   \n",
       "mean   2375.950450  2401.000000  2929.166667  110.500000    0.190380   \n",
       "std    1497.185067  1499.178146  1719.906639   64.230055    1.868504   \n",
       "min      19.000000    26.000000    12.000000    0.000000   -4.691177   \n",
       "25%    1098.750000  1131.500000  1375.500000   55.250000   -1.644746   \n",
       "50%    2239.000000  2250.500000  3068.000000  110.500000    1.036446   \n",
       "75%    3686.500000  3704.000000  4505.750000  165.750000    1.133306   \n",
       "max    5102.000000  5116.000000  5561.000000  221.000000    4.067983   \n",
       "\n",
       "                5           6  \n",
       "count  222.000000  222.000000  \n",
       "mean    -0.549550    0.096847  \n",
       "std      1.547082    0.017514  \n",
       "min     -6.000000    0.000000  \n",
       "25%     -1.000000    0.100000  \n",
       "50%      0.000000    0.100000  \n",
       "75%      0.000000    0.100000  \n",
       "max      3.000000    0.100000  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
